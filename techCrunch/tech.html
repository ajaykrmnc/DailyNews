
        <html>
        <head>
            <meta charset='utf-8'>
        </head>
        <body>
            <h1>TechCrunch </h1>
            <h2>OpenAI’s open model is delayed</h2><div class="page" id="readability-page-1"><div>
<div>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/GettyImages-2188228027_e26224.jpg"/><figcaption><strong>Image Credits:</strong>Eugene Gologursky/The New York Times / Getty Images</figcaption></figure> </div>
<div>
<p><time datetime="2025-06-10T16:34:22-07:00">4:34 PM PDT · June 10, 2025</time></p> </div>
</div><div>
<div>
<p id="speakable-summary">The release of OpenAI’s first open model in years will be delayed until later this summer, CEO Sam Altman announced in a post on X on Tuesday. Altman said the open model would be released sometime after June.</p>
<p>“[W]e are going to take a little more time with our open-weights model, i.e. expect it later this summer but not [J]une,” he wrote. “[O]ur research team did something unexpected and quite amazing and we think it will be very very worth the wait, but needs a bit longer.”</p>
<figure><div>
<blockquote data-dnt="true" data-width="500"><div dir="ltr" lang="en"><p>we are going to take a little more time with our open-weights model, i.e. expect it later this summer but not june.</p><p>our research team did something unexpected and quite amazing and we think it will be very very worth the wait, but needs a bit longer.</p></div>— Sam Altman (@sama) June 10, 2025</blockquote>
</div></figure>
<p>OpenAI was targeting an early summer release date for its open model, which is slated to have similar “reasoning” capabilities to OpenAI’s o-series of models. OpenAI aims for its open model to top the performance of other open reasoning models, such as DeepSeek’s R1.</p>
<p>In the months since OpenAI first announced its intent to release an open model, the space has become more competitive. On Tuesday, Mistral — another AI lab that often releases open models — released its first family of AI reasoning models, called Magistral. In April, the Chinese AI lab Qwen released a family of hybrid AI reasoning models that can switch off between taking time to “reason” through problems and also giving traditional, quick responses.</p>
<p>Beyond increasing its performance on benchmarks, OpenAI has also considered adding several complex features to its open AI model to make it more competitive. TechCrunch previously reported that OpenAI leaders have discussed enabling the open AI model to connect to the company’s cloud-hosted AI models for complex queries. However, it’s unclear if these features will make it into the final open model.</p>
<p>The release of OpenAI’s open model seems to be important for the company’s relationship with researchers and developers. Altman has previously said that OpenAI has landed on the “wrong side of history” when it comes to open sourcing its models. To rectify that image, the company faces immense pressure to release an open model that is competitive with the industry’s best open offerings.</p>
</div>
<div>
<p>Topics</p>
</div>
<div>
<div>
<p>
		Maxwell Zeff is a senior reporter at TechCrunch specializing in AI. Previously with Gizmodo, Bloomberg, and MSNBC, Zeff has covered the rise of AI and the Silicon Valley Bank crisis. He is based in San Francisco. When not reporting, he can be found hiking, biking, and exploring the Bay Area’s food scene.	</p>
</div>
<p>
View Bio 
</p>
</div>
</div></div><h2>Vijay Pande, founding partner of a16z bio and health strategy, steps down</h2><div class="page" id="readability-page-1"><div>
<div><p><span>In Brief</span></p><div>
<p>Posted:</p>
<p><time datetime="2025-06-10T16:07:22-07:00">4:07 PM PDT · June 10, 2025</time></p>
</div>
</div>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/vijay_pande2.jpg"/><figcaption><strong>Image Credits:</strong>Andreessen Horowitz</figcaption></figure>
<div>
<ul>
<li>
<img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/Marina-Temkin.jpg"/>
</li>
</ul>
</div>
<div>
<p id="speakable-summary">Vijay Pande, a general partner at Andreessen Horowitz who founded the firm’s a16z Bio + Health strategy, announced that he is stepping down from his role.</p>
<p>Since its founding in 2014, a16z Bio + Health has raised four funds, including a $1.5 billion fund that closed in 2022. However, it’s now seeking a much smaller $750 million fifth fund, The Wall Street Journal reported. In January, a16z Bio + Health announced that it will separately manage a $500 million biotech fund funded by pharmaceutical giant Eli Lilly.</p>
<p>a16z Bio + Health backs digital health startups and companies at the intersection of AI, computation, and biology.</p>
<p>Pande’s investments include Devoted Health, an individualized medical plan provider; Function Health, a personalized lab testing startup; and Freenome, a company aiming to detect cancer through blood draws. Before joining Andreessen Horowitz, Pande was a professor of chemistry, structural biology, and computer science at Stanford University.</p>
<p>The remaining partners on the a16z Bio + Health team are Jorge Conde, Julie Yoo, and Vineeta Agarwala.</p>
</div>
<div>
<p>Topics</p>
</div>
</div><div>
<div>
<p>Subscribe for the industry’s biggest tech news</p>
</div>
<form action="/" method="POST">
</form>
</div></div><h2>AI storage platform Vast Data aimed for $25B valuation in new round, sources say</h2><div class="page" id="readability-page-1"><div>
<p id="speakable-summary">Vast Data, which offers an AI-friendly data storage platform, is in the market to raise a new round at a giant leap in valuation.</p>
<p>Earlier this year, the 9-year-old company was seeking a valuation of around $25 billion, according to a person familiar with the deal. Should it achieve that, it would be a massive jump from its $9 billion Series E valuation secured in December 2023.</p>
<p>The deal was not finalized, and terms — including its valuation — could change, this person said, adding that the requested valuation was high at the time, despite impressive growth. Many VCs are interested in and watching Vast, other sources tell TechCrunch.</p>
<p>Vast didn’t respond to a request for comment.</p>
<p>Vast Data offers data management software coupled with unified CPU, GPU, and data hardware from vendors like Supermicro, HPE, and Cisco. Whereas old-school data storage options rely on tiers (low-cost storage options for long-term storage, higher-end options for more frequently used data), Vast aims to eliminate such tiers. It is particularly aimed at flash storage.</p>
<p>AI has been a boon to Vast’s business. The company’s platform stores structured, semi-structured, and unstructured data in one place, which accelerates data retrieval and, it says, reduces the cost of model training and inference.</p>
<p>The company’s customers include large enterprises such as Pixar, ServiceNow, and xAI, as well as next-generation AI cloud providers like CoreWeave and Lambda, which use Vast’s technology to offer storage capabilities to their end users.</p>
<p>Vast had annual recurring revenue (ARR) of $200 million when it raised its Series E about 18 months ago, TechCrunch reported. The company has been growing at 2.5x to 3x year-over-year, Renen Hallak, Vast’s CEO and co-founder, said on a podcast last May. The company has also been free cash flow positive for four years, Hallak said.</p>
<p>On data storage capabilities, Vast competes with 16-year-old publicly traded Pure Storage that has a market capitalization of nearly $17 billion, and 12-year-old Weka, which last year raised a $140 million round at a $1.6 billion valuation. Vast is also developing a database architecture that is competitive with Databricks’ offering.</p>
<p>Prior to the round it is currently working on, the company has raised a total of $381 million from investors, including Fidelity Management &amp; Research Company, NEA, BOND Capital, and Drive Capital.</p>
</div><div>
<div>
<p>
		Marina Temkin is a venture capital and startups reporter at TechCrunch. Prior to joining TechCrunch, she wrote about VC for PitchBook and Venture Capital Journal. Earlier in her career, Marina was a financial analyst and earned a CFA charterholder designation.	</p>
</div>
<p>
View Bio 
</p>
</div></div><h2>Apple Intelligence: Everything you need to know about Apple’s AI model and services</h2><div class="page" id="readability-page-1"><div>
<p id="speakable-summary">If you’ve upgraded to a newer iPhone model recently, you’ve probably noticed that Apple Intelligence is showing up in some of your most-used apps, like Messages, Mail, and Notes. Apple Intelligence (yes, also abbreviated to AI) showed up in Apple’s ecosystem in October 2024, and it’s here to stay as Apple competes with Google, OpenAI, Anthropic, and others to build the best AI tools.</p>
<h2 id="h-what-is-apple-intelligence">What is Apple Intelligence?</h2>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/wwdc24-Apple-intelligence-AI-for-the-rest-of-us-e1718051510774.jpg"/><figcaption><span><strong>Image Credits:</strong>Apple</span></figcaption></figure>
<p>Cupertino marketing executives have branded Apple Intelligence: “AI for the rest of us.” The platform is designed to leverage the things that generative AI already does well, like text and image generation, to improve upon existing features. Like other platforms including ChatGPT and Google Gemini, Apple Intelligence was trained on large information models. These systems use deep learning to form connections, whether it be text, images, video or music.</p>
<p>The text offering, powered by LLM, presents itself as Writing Tools. The feature is available across various Apple apps, including Mail, Messages, Pages and Notifications. It can be used to provide summaries of long text, proofread and even write messages for you, using content and tone prompts.</p>
<p>Image generation has been integrated as well, in similar fashion — albeit a bit less seamlessly. Users can prompt Apple Intelligence to generate custom emojis (Genmojis) in an Apple house style. Image Playground, meanwhile, is a standalone image generation app that utilizes prompts to create visual content that can be used in Messages, Keynote or shared via social media.</p>
<p>Apple Intelligence also marks a long-awaited face-lift for Siri. The smart assistant was early to the game, but has mostly been neglected for the past several years. Siri is integrated much more deeply into Apple’s operating systems; for instance, instead of the familiar icon, users will see a glowing light around the edge of their iPhone screen when it’s doing its thing.</p>
<p>More importantly, new Siri works across apps. That means, for example, that you can ask Siri to edit a photo and then insert it directly into a text message. It’s a frictionless experience the assistant had previously lacked. Onscreen awareness means Siri uses the context of the content you’re currently engaged with to provide an appropriate answer.</p>
<figure><p>
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="281" loading="lazy" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/PugKQZHPut8?feature=oembed" title="Apple Intelligence on iPhone in 5 minutes" width="500"></iframe>
</p></figure>
<p>Leading up to WWDC 2025, many expected that Apple would introduce us to an even more souped-up version of Siri, but we’re going to have to wait a bit longer.</p>
<div><p>“As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal,” said Apple SVP of Software Engineering Craig Federighi at WWDC 2025. “This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”</p><p>This yet-to-be-released, more personalized version of Siri is supposed to be able to understand “personal context,” like your relationships, communications routine, and more. But according to a Bloomberg report, the in-development version of this new Siri is too error-ridden to ship, hence its delay.</p></div>
<p>At WWDC 2025, Apple also unveiled a new AI feature called Visual Intelligence, which helps you do an image search for things you see as you browse. Apple also unveiled a Live Translation feature that can translate conversations in real time in the Messages, FaceTime, and Phone apps.</p>
<p>Visual Intelligence and Live Translation are expected to be available later in 2025, when iOS 26 launches to the public.</p>
<h2 id="h-when-was-apple-intelligence-unveiled">When was Apple Intelligence unveiled?</h2>
<p>After months of speculation, Apple Intelligence took center stage at WWDC 2024. The platform was announced in the wake of a torrent of generative AI news from companies like Google and Open AI, causing concern that the famously tight-lipped tech giant had missed the boat on the latest tech craze.</p>
<p>Contrary to such speculation, however, Apple had a team in place, working on what proved to be a very Apple approach to artificial intelligence. There was still pizzazz amid the demos — Apple always loves to put on a show — but Apple Intelligence is ultimately a very pragmatic take on the category.</p>
<p>Apple Intelligence isn’t a standalone feature. Rather, it’s about integrating into existing offerings. While it is a branding exercise in a very real sense, the large language model (LLM) driven technology will operate behind the scenes. As far as the consumer is concerned, the technology will mostly present itself in the form of new features for existing apps.</p>
<p>We learned more during Apple’s iPhone 16 event in September 2024. During the event, Apple touted a number of AI-powered features coming to its devices, from translation on the Apple Watch Series 10, visual search on iPhones, and a number of tweaks to Siri’s capabilities. The first wave of Apple Intelligence is arriving at the end of October, as part of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1.</p>
<p>The features launched first in U.S. English. Apple later added Australian, Canadian, New Zealand, South African, and U.K. English localizations. Support for Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, and Vietnamese will arrive in 2025. </p>
<h2 id="h-who-gets-apple-intelligence">Who gets Apple Intelligence?</h2>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/iPhone-15-Pro-31.jpg"/><figcaption><span><strong>Image Credits:</strong>Darrell Etherington</span></figcaption></figure>
<p>The first wave of Apple Intelligence arrived in October 2024 via iOS 18.1, iPadOS 18., and macOS Sequoia 15.1 updates. These updates included integrated writing tools, image cleanup, article summaries, and a typing input for the redesigned Siri experience. A second wave of features became available as part of iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2. That list includes, Genmoji, Image Playground, Visual Intelligence, Image Wand, and ChatGPT integration.</p>
<p>These offerings are free to use, so long as you have one of the following pieces of hardware:</p>
<ul>
<li>All iPhone 16 models</li>
<li>iPhone 15 Pro Max (A17 Pro)</li>
<li>iPhone 15 Pro (A17 Pro)</li>
<li>iPad Pro (M1 and later)</li>
<li>iPad Air (M1 and later)</li>
<li>iPad mini (A17 or later)</li>
<li>MacBook Air (M1 and later)</li>
<li>MacBook Pro (M1 and later)</li>
<li>iMac (M1 and later)</li>
<li>Mac mini (M1 and later)</li>
<li>Mac Studio (M1 Max and later)</li>
<li>Mac Pro (M2 Ultra)</li>
</ul>
<p>Notably, only the Pro versions of the iPhone 15 are getting access, owing to shortcomings on the standard model’s chipset. Presumably, however, the whole iPhone 16 line will be able to run Apple Intelligence when it arrives.</p>
<h2 id="h-how-does-apple-s-ai-work-without-an-internet-connection">How does Apple’s AI work without an internet connection?</h2>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/wwdc24-apple-intelligence-private-cloud-compute-02.jpg"/><figcaption><span><strong>Image Credits:</strong>Apple</span></figcaption></figure>
<p>When you ask GPT or Gemini a question, your query is being sent to external servers to generate a response, which requires an internet connection. But Apple has taken a small-model, bespoke approach to training. </p>
<p>The biggest benefit of this approach is that many of these tasks become far less resource intensive and can be performed on-device. This is because, rather than relying on the kind of kitchen sink approach that fuels platforms like GPT and Gemini, the company has compiled datasets in-house for specific tasks like, say, composing an email. </p>
<p>That doesn’t apply to everything, however. More complex queries will utilize the new Private Cloud Compute offering. The company now operates remote servers running on Apple Silicon, which it claims allows it to offer the same level of privacy as its consumer devices. Whether an action is being performed locally or via the cloud will be invisible to the user, unless their device is offline, at which point remote queries will toss up an error.</p>
<h2 id="h-apple-intelligence-with-third-party-apps">Apple Intelligence with third-party apps</h2>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/OpenAI-and-ChatGPT.jpeg"/><figcaption><span><strong>Image Credits:</strong>Didem Mente/Anadolu Agency / Getty Images</span></figcaption></figure>
<p>A lot of noise was made about Apple’s pending partnership with OpenAI ahead of the launch of Apple Intelligence. Ultimately, however, it turned out that the deal was less about powering Apple Intelligence and more about offering an alternative platform for those things it’s not really built for. It’s a tacit acknowledgement that building a small-model system has its limitations.</p>
<p>Apple Intelligence is free. So, too, is access to ChatGPT. However, those with paid accounts to the latter will have access to premium features free users don’t, including unlimited queries. </p>
<p>ChatGPT integration, which debuts on iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2, has two primary roles: supplementing Siri’s knowledge base and adding to the existing Writing Tools options.</p>
<p>With the service enabled, certain questions will prompt the new Siri to ask the user to approve its accessing ChatGPT. Recipes and travel planning are examples of questions that may surface the option. Users can also directly prompt Siri to “ask ChatGPT.”</p>
<p>Compose is the other primary ChatGPT feature available through Apple Intelligence. Users can access it in any app that supports the new Writing Tools feature. Compose adds the ability to write content based on a prompt. That joins existing writing tools like Style and Summary.</p>
<p>We know for sure that Apple plans to partner with additional generative AI services. The company all but said that Google Gemini is next on that list.</p>
<h2 id="h-can-developers-build-on-apple-s-ai-models">Can developers build on Apple’s AI models?</h2>
<p>At WWDC 2025, Apple announced what it calls the Foundation Models framework, which will let developers tap into its AI models while offline.</p>
<p>This makes it more possible for developers to build AI features into their third-party apps that leverage Apple’s existing systems.</p>
<p>“For example, if you’re getting ready for an exam, an app like Kahoot can create a personalized quiz from your notes to make studying more engaging,” Federighi said at WWDC. “And because it happens using on-device models, this happens without cloud API costs […] We couldn’t be more excited about how developers can build on Apple intelligence to bring you new experiences that are smart, available when you’re offline, and that protect your privacy.”<br/></p>
</div></div><h2>YouTube says its ecosystem created 490K jobs and added $55B to the US GDP in 2024</h2><div class="page" id="readability-page-1"><div>
<p id="speakable-summary">YouTube released a report on Tuesday that shows just how influential the creator economy has become.</p>
<p>YouTube says that its creative ecosystem contributed over $55 billion to the U.S. GDP and supported more than 490,000 full-time jobs, according to research by Oxford Economics.</p>
<p>When YouTube talks about its creative ecosystem, it’s not just talking about creators. This includes anyone who works with YouTube creators (video editors, assistants, publicists), as well as people who work for creator-oriented companies (Patreon, Spotter, Linktree, etc.).</p>
<p>But these figures continue to grow, even in a time when venture capitalists are no longer pouring money into the industry like they were about four years ago.</p>
<p>In 2022, YouTube and Oxford Economics reported that its creative ecosystem created about 390,000 jobs and contributed over $35 billion to the U.S. GDP, meaning that these 2024 figures jumped by 100,000 jobs and $20 billion.</p>
<p>These numbers are so large because YouTube provides the most consistent and lucrative opportunities for creators. Those who qualify for YouTube’s Partner Program can earn 55% of revenue earned from ads; even for mid-range creators (not the MrBeasts of the world), that can amount to several thousand dollars a month. While TikTok and YouTube Shorts have tried to monetize their platforms, the industry hasn’t figured out a way to reliably distribute ad revenue among short-form creators.</p>
<p>As both a fast-growing and often misunderstood sector, creators have been advocating for American institutions, from banks to the government, to better serve their industry. Some creators struggle to qualify for business credit cards or get certain business loans, regardless of their demonstrable financial solvency.</p>
<p>These issues have become common enough to draw attention. Just last week, U.S. Representatives Yvette Clark (D-NY) and Beth Van Duyne (R-TX) announced their bipartisan Congressional Creators Caucus to support and recognize the potential of the creator economy.</p>
</div><div>
<div>
<p>Amanda Silberling is a senior writer at TechCrunch covering the intersection of technology and culture. She has also written for publications like Polygon, MTV, the Kenyon Review, NPR, and Business Insider. She is the co-host of Wow If True, a podcast about internet culture, with science fiction author Isabel J. Kim. Prior to joining TechCrunch, she worked as a grassroots organizer, museum educator, and film festival coordinator. She holds a B.A. in English from the University of Pennsylvania and served as a Princeton in Asia Fellow in Laos.</p>
<p>Send tips through Signal, an encrypted messaging app, to (929) 593-0227. For anything else, email amanda@techcrunch.com.</p> </div>
<p>
View Bio 
</p>
</div></div><h2>OpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model</h2><div class="page" id="readability-page-1"><div>
<p id="speakable-summary">OpenAI has launched o3-pro, an AI model that the company claims is its most capable yet.</p>
<p>O3-pro is a version of OpenAI’s o3, a reasoning model that the startup launched earlier this year. As opposed to conventional AI models, reasoning models work through problems step by step, enabling them to perform more reliably in domains like physics, math, and coding. </p>
<p>O3-pro is available for ChatGPT Pro and Team users starting Tuesday, replacing the o1-pro model. Enterprise and Edu users will get access the week after, OpenAI says. O3-pro is also live in OpenAI’s developer API as of this afternoon. </p>
<p>O3-pro is priced at $20 per million input tokens and $80 per million output tokens in the API. Input tokens are tokens fed into the model, while output tokens are tokens that the model generates based on the input tokens.</p>
<p>A million input tokens is equivalent to about 750,000 words, a bit longer than “War and Peace.”</p>
<figure><div>
<blockquote data-dnt="true" data-width="500"><div dir="ltr" lang="en"><p>OpenAI o3-pro is available in the model picker for Pro and Team users starting today, replacing OpenAI o1-pro.</p><p>Enterprise and Edu users will get access the week after.</p><p>As o3-pro uses the same underlying model as o3, full safety details can be found in the o3 system card.…</p></div>— OpenAI (@OpenAI) June 10, 2025</blockquote>
</div></figure>
<p>“In expert evaluations, reviewers consistently prefer o3-pro over o3 in every tested category and especially in key domains like science, education, programming, business, and writing help,” OpenAI writes in a changelog. “Reviewers also rated o3-pro consistently higher for clarity, comprehensiveness, instruction-following, and accuracy.”</p>
<p>O3-pro has access to tools, according to OpenAI, allowing it to search the web, analyze files, reason about visual inputs, use Python, personalize its responses leveraging memory, and more. As a drawback, the model’s responses typically take longer than o1-pro to complete, according to OpenAI. </p>
<p>O3-pro has other limitations. Temporary chats with the model in ChatGPT are disabled for now while OpenAI resolves a “technical issue.” O3-pro can’t generate images. And Canvas, OpenAI’s AI-powered workspace feature, isn’t supported by o3-pro.</p>
<p>On the plus side, o3-pro achieves impressive scores in popular AI benchmarks, according to OpenAI’s internal testing. On AIME 2024, which evaluates a model’s math skills, o3-pro scores better than Google’s top-performing AI model, Gemini 2.5 Pro. O3-pro also beats Anthropic’s recently released Claude 4 Opus on GPQA Diamond, a test of PhD-level science knowledge.</p>
</div><div>
<div>
<p>
		Kyle Wiggers is TechCrunch’s AI Editor. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist.	</p>
</div>
<p>
View Bio 
</p>
</div></div><h2>Bedrock Ocean dredges up $25M to map the seafloor with robots</h2><div class="page" id="readability-page-1"><div>
<p id="speakable-summary">Oceans may cover more than 70% of the Earth’s surface, but we have better maps of the moon than we do the seabed. There are good reasons for that: The ocean floor is obscured, and the harsh environment makes it hard to send humans down to get a closer look. But as robots improve, we may finally get a clearer picture of the deep abyss. </p>
<p>There are a number of startups racing to map the ocean in greater detail, but the latest to snag fresh funding is Bedrock Ocean, which recently closed a $25 million Series A-2 round led by Primary and Northzone, the company exclusively told TechCrunch. Autopilot, Costanoa Ventures, Harmony Partners, Katapult, and Mana Ventures participated.</p>
<p>Bedrock Ocean has developed an autonomous underwater vehicle (AUV) that runs for up to 12 hours off its lithium-ion batteries while mapping the floor using its sonar and magnetic sensors.</p>
<p>Traditionally, the ocean floor has been mapped by large ships, which blast powerful sonar pulses down into the water column. Those ships slurp fuel and require human operators, making them costly to sail, and they disrupt marine life.</p>
<p>“The pot at the end of the rainbow that everybody has been chasing for 20 years has been, can we replace traditional ships?” Brandon Mah, COO of Bedrock Ocean, told TechCrunch.</p>
<p>Bedrock Ocean’s AUVs are still launched from a ship, but once underwater, they operate independently from it. Two of them can cover the same ground as one traditional mapping ship, and one 40-foot ship could carry 10 to 12 of the AUVs, he said.</p>
<p>Bedrock designs and builds its own AUVs, which Mah said cost less than $1 million apiece. The company also developed its own software both to operate the AUVs and to perform the mapping. The AUVs store data locally and perform some processing on board. When it’s time to transmit, they surface and send the data to the ship via Wi-Fi. A Starlink antenna aboard the ship can then beam that information to the cloud, where observers can keep an eye on things.</p>
<p>“We can confirm that the data is of the quality that we’re targeting, as well as identify potential targets that we want to investigate further in near real time,” Mah said.</p>
<p>The subs sail five to 10 meters above the seafloor, allowing the AUVs to use less powerful sonar than ship-based mapping would. Mah said that Bedrock’s sonar poses less harm to marine mammals because its frequency is outside the audible range, uses less power, and is deployed closer to the seafloor, meaning the animals are less likely to be swimming in the sonar’s path.</p>
<p>Bedrock can place items on the ocean floor with an accuracy of one to two meters. That’s below the sub-meter accuracy ship-based surveys can deliver, and it’s because the AUVs lose GPS signals after they dive. To determine their position underwater, they rely on inertial navigation, which isn’t as accurate as GPS.</p>
<p>Mah argues that not every survey needs sub-meter accuracy, but that many operations on the seafloor would benefit from faster mapping. Offshore wind developers might pay for a sub-meter survey up front, but then when it comes time to build, there might be areas where two-meter accuracy is enough.</p>
<p>The startup has spent the last two quarters doing paid survey work for offshore wind, oil and gas, and environmental assessments. The speed of the assessments, and the ability to view the data quickly, has also caught the attention of the U.S. Navy. “We showed off that capability,” Mah said. “They were kind of blown away.”</p>
<p><em>Update 7 pm ET: Brandon Mah is COO of Bedrock Ocean, not CEO as previously stated.</em></p>
</div><div>
<div>
<p>
		Tim De Chant is a senior climate reporter at TechCrunch. He has written for a wide range of publications, including Wired magazine, the Chicago Tribune, Ars Technica, The Wire China, and NOVA Next, where he was founding editor. De Chant is also a lecturer in MIT’s Graduate Program in Science Writing, and he was awarded a Knight Science Journalism Fellowship at MIT in 2018, during which time he studied climate technologies and explored new business models for journalism. He received his PhD in environmental science, policy, and management from the University of California, Berkeley, and his BA degree in environmental studies, English, and biology from St. Olaf College.	</p>
</div>
<p>
View Bio 
</p>
</div></div><h2>Whole Foods warns of shortages after cyberattack at its primary distributor UNFI</h2><div class="page" id="readability-page-1"><div>
<p id="speakable-summary">Whole Foods told its employees that the ongoing outages and disruptions at its primary distributor, United Natural Foods (UNFI), may take “several days to resolve.”</p>
<p>The Amazon-owned retail giant told staff in an internal communication, seen by TechCrunch, that UNFI was experiencing a “nationwide technology system outage,” which UNFI has for its part described as a cybersecurity incident. </p>
<p>Whole Foods said in the communication to staff that the cyberattack is affecting UNFI’s “ability to select and ship products from their warehouses,” and that this will “impact our normal delivery schedules and product availability.” </p>
<p>The missive to staff included instructions to limit communications with customers. The “only single approved customer talking point” that Whole Foods employees can share with customers, according to the communication, is that the grocery giant is having “temporary supply challenges.”</p>
<p>When reached by TechCrunch, Whole Foods spokesperson Nathan Cimbala said: “We are working to restock our shelves as quickly as possible and apologize for any inconvenience this may have caused for customers.”</p>
<p>UNFI has not responded to TechCrunch’s request for comment on Tuesday, nor given a timeline for its recovery. Whole Foods’ spokesperson did not say how the company reached its claim that the situation may resolve in a few days.</p>
<p>UNFI is one of the largest food distributors in North America, supplying grocery goods and fresh produce to more than 30,000 stores and supermarkets across the U.S. and Canada. The company disclosed the cyberattack on Monday in a filing with federal regulators, and UNFI’s chief executive, Sandy Douglas, told investors Tuesday that the company took its entire network offline on Friday after detecting the intrusion. </p>
<p>The company also on Tuesday reported $8.1 billion in net sales in the quarter ended May 3, 2025.</p>
<p>As we reported earlier Tuesday, TechCrunch has heard anecdotal reports of empty shelves at some Whole Foods stores and other grocery stores reliant on UNFI. </p>
<p>A Whole Foods store visited by this reporter on Tuesday displayed notices in several aisles saying that the store was experiencing an unspecified “temporary out of stock issue” for some products. </p>
<p>Much of the downstream real-world impact on grocery stores and their customers may not be seen until later this week.</p>
<p><em>Do you know more about the cyberattack at UNFI? Are you a corporate customer affected by the disruption? You can securely contact this reporter via encrypted message at zackwhittaker.1337 on Signal.</em></p>
</div><div>
<div>
<p>
		Zack Whittaker is the security editor at TechCrunch. He can be reached via encrypted message at zackwhittaker.1337 on Signal, or by email at zack.whittaker@techcrunch.com. 	</p>
</div>
<p>
View Bio 
</p>
</div></div><h2>Why VCs should care about TechCrunch All Stage 2025</h2><div class="page" id="readability-page-1"><div>
<div>
<figure><img src="/Users/ajaymac/Desktop/raw project/DailyNews/DailyNews/extractFunction/../images/53680847826_492c237136_o.jpg"/><figcaption><strong>Image Credits:</strong>Halo Creative</figcaption></figure> </div>
<div>
<p><time datetime="2025-06-10T11:40:00-07:00">11:40 AM PDT · June 10, 2025</time></p> </div>
</div><div>
<div>
<p id="speakable-summary">Let’s be honest: Most events aren’t built for VCs — they’re built around VCs. Panels, keynotes, maybe a few startups worth watching, and a sea of business cards you’ll forget by dinner.</p>
<p>TechCrunch All Stage 2025 flips that script, when we fill up Boston’s SoWa Power Station on July 15. This isn’t just a chance to speak onstage and bounce — it’s an opportunity to connect with high-caliber founders, early-stage operators, and other investors who are laser-focused on execution. If you’re looking to expand your deal flow, spot overlooked sectors, and meet the next breakout team before their Series A memo is half written, this is the room to be in.</p>
<p>And now is a better time than ever to snag a ticket — we have a limited-time $200 discount on Investor passes, which give you the full access to our list of founders, our full lineup of events and speakers, plus session docs and recordings for future reference.</p>
<h2 id="h-the-perks-of-being-a-vc-at-techcrunch-all-stage">The perks of being a VC at TechCrunch All Stage</h2>
<ul>
<li>Curated networking opportunities (yes, actually useful)</li>
<li>Access to founder-focused sessions that show you what startups are really dealing with</li>
<li>A more intimate, one-day format — no filler, no fluff</li>
</ul>
<p>And our lineup of speakers aren’t resting on their laurels or their reputation. Everyone onstage is sharing their playbooks, mistakes, and what they wish more founders knew:</p>
<ul>
<li>Cathy Gao of Sapphire Ventures on what scale-stage execution actually looks like</li>
<li>Charles Hudson from Precursor Ventures on investing in conviction early, not consensus late</li>
<li>Tiffany Luck with NEA on what to expect when raising from top-tier firms in this environment</li>
</ul>
<p>Founders are coming to All Stage to learn, build, and grow — but this is your chance to meet and identify the ones with the skills, the drive, and the innovation needed to make your next partnership and investment a massive success. </p>
<p>Don’t miss out on your next investment opportunity — join us July 15 at SoWa Power Station in Boston. <strong>Register now</strong>!</p>
</div>
<div>
<p>Topics</p>
</div>
</div></div>
        </body>
        </html>
    